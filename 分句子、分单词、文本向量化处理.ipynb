{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed05e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "doc = \"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\"\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edd33e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Are you curious about tokenization?\n",
      "2 : Let's see how it works!\n",
      "3 : We need to analyze a couple of sentences with punctuations to see it in action.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "分句子\n",
    "\"\"\"\n",
    "sents = nltk.sent_tokenize(doc)\n",
    "for i in range(len(sents)):\n",
    "    print(i+1,\":\",sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c0585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Are\n",
      "2 : you\n",
      "3 : curious\n",
      "4 : about\n",
      "5 : tokenization\n",
      "6 : ?\n",
      "7 : Let\n",
      "8 : 's\n",
      "9 : see\n",
      "10 : how\n",
      "11 : it\n",
      "12 : works\n",
      "13 : !\n",
      "14 : We\n",
      "15 : need\n",
      "16 : to\n",
      "17 : analyze\n",
      "18 : a\n",
      "19 : couple\n",
      "20 : of\n",
      "21 : sentences\n",
      "22 : with\n",
      "23 : punctuations\n",
      "24 : to\n",
      "25 : see\n",
      "26 : it\n",
      "27 : in\n",
      "28 : action\n",
      "29 : .\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "分单词\n",
    "\"\"\"\n",
    "words = nltk.word_tokenize(doc)\n",
    "for i in range(len(words)):\n",
    "    print(i+1,\":\",words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974ea48",
   "metadata": {},
   "source": [
    "### 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "312cc172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "sents = [\"This hotel is very bad\",\n",
    "        \"The toilet in this hotel smells bad. \",\n",
    "        \"The environment of this hotel is very good.\"]\n",
    "import sklearn.feature_extraction.text as ft\n",
    "cv = ft.CountVectorizer()\n",
    "bow = cv.fit_transform(sents)\n",
    "\"\"\"\n",
    "以下输出的输出结果是稀疏矩阵\n",
    "\"\"\"\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfd7e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 1 0 0 0 1 0 1]\n",
      " [1 0 0 1 1 0 0 1 1 1 1 0]\n",
      " [0 1 1 1 0 1 1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "获得原始矩阵\n",
    "\"\"\"\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36418371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'environment', 'good', 'hotel', 'in', 'is', 'of', 'smells', 'the', 'this', 'toilet', 'very']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "获得特征名称\n",
    "\"\"\"\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee241de",
   "metadata": {},
   "source": [
    "### TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2c8d76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t0.4875913479575303\n",
      "  (0, 9)\t0.37865817843463756\n",
      "  (0, 5)\t0.4875913479575303\n",
      "  (0, 3)\t0.37865817843463756\n",
      "  (0, 0)\t0.4875913479575303\n",
      "  (1, 10)\t0.45386826657073503\n",
      "  (1, 9)\t0.2680619096684997\n",
      "  (1, 8)\t0.34517851538731575\n",
      "  (1, 7)\t0.45386826657073503\n",
      "  (1, 4)\t0.45386826657073503\n",
      "  (1, 3)\t0.2680619096684997\n",
      "  (1, 0)\t0.34517851538731575\n",
      "  (2, 11)\t0.32628713817645505\n",
      "  (2, 9)\t0.2533910700140413\n",
      "  (2, 8)\t0.32628713817645505\n",
      "  (2, 6)\t0.4290283757733418\n",
      "  (2, 5)\t0.32628713817645505\n",
      "  (2, 3)\t0.2533910700140413\n",
      "  (2, 2)\t0.4290283757733418\n",
      "  (2, 1)\t0.4290283757733418\n"
     ]
    }
   ],
   "source": [
    "tt = ft.TfidfTransformer()\n",
    "tfidf = tt.fit_transform(bow)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6e31c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49, 0.  , 0.  , 0.38, 0.  , 0.49, 0.  , 0.  , 0.  , 0.38, 0.  ,\n",
       "        0.49],\n",
       "       [0.35, 0.  , 0.  , 0.27, 0.45, 0.  , 0.  , 0.45, 0.35, 0.27, 0.45,\n",
       "        0.  ],\n",
       "       [0.  , 0.43, 0.43, 0.25, 0.  , 0.33, 0.43, 0.  , 0.33, 0.25, 0.  ,\n",
       "        0.33]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(tfidf.toarray(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7709e325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad',\n",
       " 'environment',\n",
       " 'good',\n",
       " 'hotel',\n",
       " 'in',\n",
       " 'is',\n",
       " 'of',\n",
       " 'smells',\n",
       " 'the',\n",
       " 'this',\n",
       " 'toilet',\n",
       " 'very']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558b7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
